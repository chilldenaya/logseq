- Latar Belakang:
	- text summarization adalah
	- ada dua jenis, abstractive dan extractive
	- abstraktif memiliki kecenderungan untuk memberikan hasil yang buruk jika diberikan masukan dan keluaran dokumen yang panjang
	- biasanya yang lebih digunakan adalah extractive
	- beberapa jenis extractive text summarization sudah ada
	- yang populer adalah metode neural netowrk
	- Extractive text summarization dapat dipandang sebagai sequential labelling dengan label tiap token adalah biner
	- tapi, karena dipandang sebagai sequential labelling, perlu anotasi data latih (labelling training set)
	- membuat satu dokumen dengan N token memerlukan N anotasi label.
	- cara menghindari ini adalah dengan menggunakan metode unsupervised
	- salah satu cara yang digunakan adalah deep q network -> sentence and document encoder + reinforcement learning
		- apa boleh langsung begini? apa perlu mencaritahu kenapa pakai reinforcement learning? kenapa bukan metode lain yang juga bisa unsupervised?
		  id:: 624c518e-af6a-4581-a03d-4ce9804c8e0c
	- di DQN, sentence encoder ny menggunakan RNN/CNN
	- tapi masalahnya apa?
		- ini belum nemu masalah kenapa RNN atau CNN are worse than transformers like BERT
	- maka dari itu penulis mencoba menggunakan sentenceBERT + Q-learning untuk proses unsupervised extractive summarization
	-
- Bab II
	- II.1 Text Summarization
		- II.1.1 Abstractive Summarization
		- II.1.2 Extractive Summarization
	- II.2 Sentence Encoder
	- II.3 Reinforcement Learning
	- II.4 Deep Q Network
		- II.4.1 RNN Sentence Encoder
		- II.4.2 CNN Sentence Encoder
		- II.4.3 Document Encoder
	-
	-
		-
-
-
- [[Bimbingan]]
-